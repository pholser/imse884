LP background
=====
max z_LP = c^T * x
s.t. Ax <= b
     x >= 0

c, x in R^n
A in R^(m x n)
b in R^(m x 1)

Integer program:
max z_IP = c^T * x
s.t. Ax <= b
     x in Z^n_+   (x >= 0 and integer)

c in R^n
A in R^(m x n)
b in R^(m x 1)

Is there a relationship between z_LP and z_IP?
z_LP >= z_IP

In general, no poly-time algo to solve an IP.

Treat the LP as a linear relaxation of the IP
(remove the integrality restrictions).

e.g.
max 5x_1 + 2x_2
s.t. 3x_1 + x_2  <= 10
     2x_1 + 3x_2 <=  8
     x_1, x_2 >= 0 and integer

Linear relaxation:
max 5x_1 + 2x_2
s.t. 3x_1 + x_2  <= 10
     2x_1 + 3x_2 <=  8
     x_1, x_2 >= 0

Convex problems easy to solve; non-convex
problems not so much.

-- Convexity of function
    f(x) is convex
    <->
    forall x_1, x_2 in dom(f), forall lambda in [0, 1]:
        f(lambda * x_1 + (1 - lambda) * x_2)
        <=
        lambda * f(x_1) + (1 - lambda) * f(x_2)

-- Convexity of set
    S subset-of R^n is convex
    <->
    forall x_1, x_2 in S, forall lambda in [0, 1]:
        lambda * x_1 + (1 - lambda) * x_2 in S

-- feasible region of an LP is a convex set;
    IP not so much, unless:
    -- feasible region is a singular point
    -- feasible region is empty set

The convex hull of S is the intersection of all
convex sets that contain S.

Can the linear relaxation of an IP be feasible,
but the IP infeasible?
--> max x_1 + x_2
    s.t. x_1 <= 1/2
         x_1 >= 1/4
    (linear relaxation unbounded here.)
--> narrow triangle that never contains integer points

Extreme points of feasible region of LP:
-- If there is an optimum, it's at a corner point
   -- As long as LP not infeasible or unbounded
-- Also called basic feasible solutions (BFS)

Degeneracy:
-- > 1 basis corresponding to a single BFS
-- One of the basic vars takes zero
    (or takes on lower/upper bound of var)

Degeneracy created by:
-- Redundant constraints
-- > n hyperplanes meeting at a point in n
    dimensions

Simplex method: move from corner to corner,
till optimum found.

e.g. max 5x_1 + 3x_2
     s.t. x_1 <= 4
          -x_1 + x_2 <= 6
          3x_1 + 2x_2 <= 18
          x_1, x_2 >= 0

z   x_1   x_2   s_1   s_2   s_3    RHS
--------------------------------------
1    -5    -3     0     0     0      0
0     1     0     1     0     0      4  s_1
0    -1     1     0     1     0      6  s_2
0     3     2     0     0     1     18  s_3

x_1 leaves
s_1 enters: 4/1 min

z   x_1   x_2   s_1   s_2   s_3    RHS
--------------------------------------
1     0    -3     5     0     0     20
0     1     0     1     0     0      4  x_1
0     0     1     1     1     0     10  s_2
0     0     2    -3     0     1      6  s_3

x_2 leaves
s_3 enters: 6/2 min

z   x_1   x_2   s_1   s_2   s_3    RHS
--------------------------------------
1     0     0    1/2    0   3/2     29
0     1     0     1     0     0      4  x_1
0     0     0    5/2    1   -1/2     7  s_2
0     0     1   -3/2    0    1/2     3  x_2

z* = 29, x_1 = 4, x_2 = 3, s_2 = 3/2, s_1 = 0, s_3 = 0

If this were an IP, and we ended up with a non-integer
soln, we'd need to cut the space down.

Add a constraint: x_1 + x_2 <= 5
Re-solve as LP
Add constraint to tableau
Re-establish basis...
Check optimal; if not, pivot, maybe dual simplex


Duality in LP
=====

primal:
e.g.
max c^T * x
s.t. Ax <= b
     x >= 0

dual:
min b^T * y
s.t. A^T * y >= c
    y >= 0

c^T * x <= b^T * y   # weak duality

Strong duality theorem:
If there is an optimum to the primal,
there is an optimum to the dual, and
    c^T * x^* = b^T * y^*

Complementary slackness:
For a corner-point soln:
If x_i is basic, e_i is non-basic
    (e = surplus var)
If s_i is basic, y_i is non-basic
    (s = slack var)

    x * e = 0
    s * y = 0

Reduced costs/shadow prices/dual prices: values in
objective row of simplex tableau above the slacks

Sensitivity analysis

Better off with simplex method on linear relaxations
of IP, as opposed to interior-point methods
(why?)

Simplex may require exponential time in pathological
cases.
There are polynomial-time algos for LP now
(interior-point methods)

if A then B:
T     T   -->  T  # Assume A true; show B must be true
T     F   -->  F
F     T   -->  T
F     F   -->  T

A iff B:
T     T   -->  T  # Assume A true; show B must be true
T     F   -->  F
F     T   -->  F
F     F   -->  T  # Assume B true; show A must be true

Proof by contradiction of "if A then B":
-- Assume A is true; assume B is false.
-- What follows from this? Need to show that
    this leads to a contradiction, and so B
    must be true

Contrapositive: if ~B, then ~A
(A -> B is equivalent to ~B -> ~A)

Converse: if B, then A
Inverse: if ~A, then ~B


Graph: set G = (V, E)
V = vertices, E = edges

Directed edges or undirected edges

Graph as substructure for building an IP

Node-node adjacency matrix representation
(A[x, y] = 1 if nodes x and y are connected via
an edge, else ___? (sometimes 0, sometimes -(degree)))

   A   B   C   D
A  0   1
B  1   0
C          0
D              0

(symmetric for undirected graph)

Node-arc incidence matrix:
    (A,B) (A,C) ...
A
B
C
C

Exactly two ones in every column
Usually more edges than nodes -> more storage req'd

Directed node-arc rep:
 1 for flow-out
-1 for flow-in
All columns sum to zero --> unimodular matrix
-- not linearly independent
    --> cross off bottom row
        --> basis: tree (graph w/no cycles)
            fast pivots (network simplex method)
-- If this is only set of constraints, and RHS
    of constraints are integers, then you'll get
    an integer soln always

Paths

Discovered tree

Clique of a graph: subgraph that has all possible
edges (every node has an edge to every other node)
-- k_n, where n is # of nodes in clique
(complete graph)
# edges: C(n, 2)

Complement of a graph:
graph with same set of vertices, but with edges
where there weren't edges in original graph, and
the original edges removed.

Bipartite graph:
where vertices are partitioned into two sets, and
there are no edges between two nodes in the same
partition.
These come up in transportation models a lot--
-- sources vs. destinations
-- assignment problems: tasks to workers...
-- no odd cycles

Matching problem:
-- sum[i]( x_ij ) <= 1 forall j
-- sum[j]( x_ij ) <= 1 forall i

Node packing problem:
find a set of nodes of maximum cardinality such that
no node is incident with the same edge (or arc)

Set packing problem:
Given a universe U and a family S of subsets of U,
find a collection C of sets in S, whose cardinality is
maximized, such that all sets in C are pairwise
disjoint.

O(f(n)), Omega(f(n)), Theta(f(n))
Polynomial-in-n time

Implicit enumeration of binary IP solutions:
exponential in # vars

NP: set of all problems whose solutions can be verified
in polynomial time
NP-complete: set of NP problems that can be transformed
into each other in polynomial time.
NP-hard: at least as hard as the hardest problems in NP
(don't have to be elements of NP)

IP are NP-complete problems

Two important sets in an IP:
-- Convex hull of feasible integer points
-- Region induced by the (linear relaxation of)
    the constraints

IP:
max c^T * x
s.t. Ax <= b
    x >= 0

Let P = { x in Z^n_+ : Ax <= b }
(countable set of points)
Let P_LR = { x in R^n_+ = Ax <= b }
    (linear relaxation set)
Let P_CH = convex hull of P

Every LP has an optimum, if it does, at an
extreme point/BFS/corner point

If we change the IP to:
max c^T * x
s.t. x in P_CH,
then an optimum will be at a corner point of
P_CH

Main idea: To LR, add a bunch of other constraints
such that we whittle the feasible region down in
this way such that we obtain corner points that
are integer --> cutting planes

P_CH and P_LR are polyhedra
{ x in R^n : Ax <= b }
* All polyhedra are convex.

Convex set that is not a polyhedron: circle

Adding cutting planes iteratively, it introduces
the risk of introducing degeneracy.

backwards-e == "such that"


Branch-and-bound for IP
=====
-- graphical
-- knapsack

0) Initialization: Create a search tree where the nodes are
LP relaxation problems. The tree starts with one node (the root),
which is the relaxation of the original LP.
Set Z*_ip = -inf.

While there exists at least one unfathomed leaf node:
-- Solve the LP relaxation of this node with solution
    z_LP and x_LP, then perform one of the following
    four steps:

1) If x_LP is integer, then an integer solution has been
found. If z_LP > z*_IP, then set z*_IP to z_LP and save
x_LP as the corresponding soln; fathom this node.

2) If z_LP <= z*_IP, then fathom this node (known as the
bounding portion).

3) If the LP relaxation is infeasible, then fathom this
node.

4) If z_LP >= z*_IP and x_LP is not integer, then there exists some
p < x_i < p+1 where p in Z. Create two new nodes to the tree;
these nodes are the children of the node that we are working on.
The LP relaxation problem for one of these child is the
LP relaxation of the parent with an additional constraint of
x_i <= p. The other child node also has the parent's LP relaxation
and includes the constraint x_i >= p + 1.

Can traverse the tree:
-- breadth-first
    -- left child first
    -- right child first
-- depth-first
    -- left child first
    -- right child first
-- best child
-- best bound

What var to branch on?
-- for exam: choose lowest index candidate var

Need to be able to traverse from leaf thru ancestry,
to pick up all the constraints

Track basis, z-value, ...

Running time for a branch-and-bound IP algo:
-- # branches gets large quickly
-- # nodes at level n: 2^n

0-1 knapsack problems:
max c^T * x
s.t. a^t * x <= b
    x in {0, 1}^n
    a >= 0

e.g.
max 5x_1 + 8x_2 + 12x_3 + 6x_4 + 3x_5
s.t. x_1 + 2x_2 + 4x_3 + 3x_4 + 3x_5 <= 9
    x_i in {0, 1}

c = values of items
a = weights of items
b = capacity of knapsack

2^n candidate solutions.

dual:
min 9y
s.t. y >= 5
     2y >= 8
     4y >= 12
     3y >= 6
     3y >= 3
     y >= 0

min ratio test: c_i/a_i

Add bounds between 0 and 1 on all the x_i.
Pivot on x_1 (max c_i/a_i)

z   x_1   x_2   x_3   x_4   x_5   s_i  RHS
==========================================
1    -5   -8    -12    -6    -5    0    0
0     1    2      4     3     3    1    9
   add constraint
      1                                 1

Set x_1 to upper bound, 1.
z now 5
Now move x_2 to its upper bound, 1
z now 13
...

greedily choosing stuff with best value/weight ratio
until you have to take a fractional amount, or you're
at (integer) capacity

If something's at its upper bound, negative is OK in
objective row of tableau; if at its lower bound, positive
OK.

If upper bound is lower than ratio test result, take
the upper bound as pivot

Branch-and-bound on above example, using best-child
strategy.

e.g.
max 5x_1 + 8x_2 + 12x_3 + 6x_4 + 3x_5
s.t. x_1 + 2x_2 + 4x_3 + 3x_4 + 3x_5 <= 9
    x_i in {0, 1}

Node root)
(1, 1, 1, 2/3, 0)
z = 29

Branch: node 1) x_4 <= 0 vs. node 2) x_4 >= 1

Best child:
Node 1): z = 27 (1, 1, 1, 0, 2/3)
Node 2): z = 28 (1, 1, 3/4, 1, 0)

Node 2 is best since z is greater.
Branch from here: node 3) x_3 <= 0, node 4) x_3 >= 1
Node 3): z = 22 (1, 1, 0, 1, 1)
    --> z_best = 22
Node 4): z = 27 (1, 1/2, 1, 1, 0)
Best child is a tie, choose node 5) for branch
Node 6):
...

Branching on non-fractional vars?

Choosing between vars to branch on?
-- HW: "cheat". Find whatever amounts to less work.


Advanced branching strategies:
With an IP of 100 vars, branching tree could get
to be 2^100 nodes

Best-child advantageous re: intelligently pruning
search space.

DFS advantageous re: memory management
randomly choose left/right

BFS: memory problems

Best-child: sorting children. Priority queue

The shape of the problem matters.
1) Feasibility problem
2) Optimality problem
3) Mixture of 1) and 2)

Random diving

-- If A and b are rational
-- If there is a feasible integer point
-- If LP unbounded
-- Then IP is unbounded

Pseudo-branching:

We would be branching on a basic var; never
a non-basic var (NBV would always be zero, an integer)

We know based on the reduced costs of NBV, which vars are
doing more or less harm in achieving optimality.

What if we branched on a BV based on some estimate of
reduced cost --> Pseudo-cost.

Gets a broader separation, where one child of the branch
improves a lot, the other doesn't --> hopefully fathom
larger chunk of tree.

No silver bullet, that will work well on every problem.

Strong branching:
-- n vars, assume they're binary; bad reduced costs
    call that set of vars S

root: branch on sum[x_i in S](x_i) = 0, and on
    sum[x_i in S](x_i) >= 1

Hopefully fathom one side or another quickly.

Or:
root: branch on sum[x_i in S](x_i) = n, and on
    sum[x_i in S](x_i) <= n - 1
where n is |S|

-- branch on var least close to integer
(CPLEX default)
    -- depends on the problem
        -- feasibility vs. optimality

Rules for branching:
1) Between all children, no feasible integer points
eliminated
2) Must remove linear relaxation of parent
3) Must eventually terminate with only integer
    extreme points
    -- End at a single integer point

Branching on polyhedra:
-- Each node with four children:
-- Only works for integer problems, not binary

Easton: Branching: easier and better than many other
techniques for solving IP
-- Sometimes helpful to invent hand-rolling branching
strategy for solving specific problems


Column generation:
When LP/IP has way more variables than constraints
-- Comes up a lot in crew scheduling problems

Idea: take the problem, cut it down to some reasonable
size involving fewer vars; introduce new columns as you
go
-- Don't add variables that would never pivot in

The overarching idea is that many linear programs are too large
to consider all the variables explicitly. Since most of the
variables will be non-basic and assume a value of zero in the
optimal solution, only a subset of variables need to be considered
in theory when solving the problem. Column generation leverages
this idea to generate only the variables which have the potential
to improve the objective function -- that is, to find variables
with negative reduced cost (assuming without loss of generality that
the problem is a minimization problem).

The problem being solved is split into two problems:
the master problem and the subproblem. The master problem is
the original problem with only a subset of variables being considered.
The subproblem is a new problem created to identify a new variable.
The objective function of the subproblem is the reduced cost of
the new variable with respect to the current dual variables,
and the constraints require that the variable obey the naturally
occurring constraints.

The process works as follows. The master problem is solved --
from this solution, we are able to obtain dual prices for each of
the constraints in the master problem. This information is then
utilized in the objective function of the subproblem.
The subproblem is solved. If the objective value of the subproblem
is negative, a variable with negative reduced cost has been
identified. This variable is then added to the master problem,
and the master problem is re-solved. Re-solving the master problem
will generate a new set of dual values, and the process is repeated
until no negative reduced cost variables are identified.
The subproblem returns a solution with non-negative reduced cost,
we can conclude that the solution to the master problem is optimal.
































.
